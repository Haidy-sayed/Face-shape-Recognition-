{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29917,"status":"ok","timestamp":1667152460780,"user":{"displayName":"Salma Hussen","userId":"00935840422521980055"},"user_tz":-120},"id":"IlFB92ZVt4ei","outputId":"cf0a2638-33a3-4dba-f15a-db13fff2a46b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting face_recognition\n","  Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: Click\u003e=6.0 in /usr/local/lib/python3.7/dist-packages (from face_recognition) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from face_recognition) (1.21.6)\n","Collecting face-recognition-models\u003e=0.3.0\n","  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n","\u001b[K     |████████████████████████████████| 100.1 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: dlib\u003e=19.7 in /usr/local/lib/python3.7/dist-packages (from face_recognition) (19.24.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from face_recognition) (7.1.2)\n","Building wheels for collected packages: face-recognition-models\n","  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566186 sha256=9f2219bc599851f8bc84294efe319e88ce5414c151426f788a207b16fe739865\n","  Stored in directory: /root/.cache/pip/wheels/d6/81/3c/884bcd5e1c120ff548d57c2ecc9ebf3281c9a6f7c0e7e7947a\n","Successfully built face-recognition-models\n","Installing collected packages: face-recognition-models, face-recognition\n","Successfully installed face-recognition-1.3.0 face-recognition-models-0.3.0\n"]}],"source":["!pip install face_recognition"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"kmFrDtm8tjXC"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-2-56861667c3ea\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mresults_visualization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'results_visualization'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}],"source":["# Import packages\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten, Dropout, MaxPooling2D, BatchNormalization, Conv2D\n","from tensorflow.keras import optimizers\n","from sklearn.metrics import accuracy_score\n","import numpy as np\n","\n","from results_visualization import plot_history, plot_confusion_matrix\n","\n","\n","class B1:\n","    def __init__(self, input_shape):\n","        \"\"\"\n","        The network consists in 3 consecutive convolutional blocks (CONV-\u003ePOOL) followed by a dense layer and\n","        a softmax classifier. Dropout and Batch normalization are applied to enhance the model performance.\n","\n","        :param input_shape: size of the first layer input\n","        \"\"\"\n","        self.model = Sequential([\n","            Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same', input_shape=input_shape),\n","            MaxPooling2D(pool_size=(2, 2), strides=2),\n","            Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'),\n","            MaxPooling2D(pool_size=(2, 2), strides=2),\n","            Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'),\n","            BatchNormalization(),\n","            MaxPooling2D(pool_size=(2, 2), strides=2),\n","            Flatten(),\n","            # Fraction of the input units dropped\n","            Dropout(rate=0.5),\n","            # Number of units equal to the number of classes\n","            Dense(units=5, activation='softmax')\n","        ])\n","        self.model.summary()\n","        # Configures the model for training.\n","        self.model.compile(optimizer=optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy',\n","                           metrics=['accuracy'])\n","\n","    def train(self, training_batches, valid_batches, epochs=15, verbose=1, plot=True):\n","        \"\"\"\n","        Trains the model for a fixed number of iterations on the entire dataset (epochs).\n","\n","        :param training_batches: input data given in batches of n examples.\n","        :param valid_batches: batches of examples on which to evaluate the loss and model metrics after each epoch.\n","            The model is not trained on them.\n","        :param epochs: number of epochs to train the model. default_value=15\n","        :param verbose: verbosity level. default_value=1.\n","        :param plot: if True it plots the learning and performance curves. default_value=True\n","        :return: the last accuracies measured on the training and validation sets.\n","        \"\"\"\n","        # Parameters needed because fit() will run forever since image_generator.flow_from_dataframe()\n","        # is a infinitely repeating dataset\n","        history = self.model.fit(x=training_batches,\n","                                 steps_per_epoch=len(training_batches),\n","                                 validation_data=valid_batches,\n","                                 validation_steps=len(valid_batches),\n","                                 epochs=epochs,\n","                                 verbose=verbose\n","                                 )\n","        if plot:\n","            # Plot loss and accuracy achieved on training and validation dataset\n","            plot_history(history.history['accuracy'], history.history['val_accuracy'], history.history['loss'],\n","                         history.history['val_loss'])\n","        # Return accuracy on the train and validation dataset\n","        return history.history['accuracy'][-1], history.history['val_accuracy'][-1]\n","\n","    def test(self, test_batches, verbose=1, confusion_mesh=True, class_labels='auto'):\n","        \"\"\"\n","        Generates output predictions for the input examples and compares them with the true labels returning\n","        the accuracy gained.\n","\n","        :param test_batches: input data given in batches of n examples taken from the test dataset.\n","        :param verbose: verbosity level. default_value=1\n","        :param confusion_mesh: if True it plots the confusion matrix. default_value=True\n","        :param class_labels: list of the class names used in the confusion matrix. default_value='auto'\n","        :return: the test accuracy score\n","        \"\"\"\n","        # Steps parameter indicates how many batches are necessary to work on each data in the testing dataset\n","        # model.predict returns the predictions made on the input given\n","        # It returns the probabilities that each image belongs to the existing classes\n","        predictions = self.model.predict(x=test_batches, steps=len(test_batches), verbose=verbose)\n","        # Transform each prediction to an hot-encoding vector\n","        predictions = np.round(predictions)\n","        # The image is associated to the class with the highest probability\n","        predicted_labels = np.array(np.argmax(predictions, axis=-1))\n","        # Retrieve the true labels of the input\n","        true_labels = np.array(test_batches.classes)\n","        # Plot results through a confusion matrix\n","        if confusion_mesh:\n","            plot_confusion_matrix(class_labels, predicted_labels, true_labels)\n","        # Return accuracy on the test dataset\n","        return accuracy_score(true_labels, predicted_labels)\n","\n","    def evaluate(self, test_batches, verbose=1):\n","        \"\"\"\n","        Displays the metrics and the loss values of the model tested.\n","\n","        :param test_batches: input data given in batches of n examples taken from the test dataset.\n","        :param verbose: verbosity level. default_value=1.\n","        :return: print the score achieved\n","        \"\"\"\n","        # model.evaluate predicts the output and returns the metrics function specified in model.compile()\n","        score = self.model.evaluate(x=test_batches, verbose=verbose)\n","        print(score)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417},"executionInfo":{"elapsed":399,"status":"error","timestamp":1667134775330,"user":{"displayName":"Salma Hussen","userId":"00935840422521980055"},"user_tz":-120},"id":"b_vi6rzyvoGI","outputId":"44e669d2-c1ba-43a1-ccec-809e896c7f27"},"outputs":[{"name":"stdout","output_type":"stream","text":["Num GPUs Available:  1\n"]},{"ename":"FileNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-3-e02c950bdee7\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# A1 ===================================================================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Extract smiles for A2 task before dividing all the images in 'celeba' in training and test images.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 20\u001b[0;31m \u001b[0mdata_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfaces_not_detected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmiles_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'celeba'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mtest_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfaces_not_detected1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmiles_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'celeba_test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/face_extraction.py\u001b[0m in \u001b[0;36msmiles_extraction\u001b[0;34m(dataset_name, img_size)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaces_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# copy the labels.csv file into the new folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 29\u001b[0;31m     \u001b[0mcopy2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;31m# List of all the images available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mcopy2\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 266\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m     \u001b[0mcopystat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 120\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Datasets/celeba/labels.csv'"]}],"source":["# Import packages\n","from delete_glasses import delete_glasses\n","from face_extraction import smiles_extraction\n","from pre_processing import data_preprocessing, hog_pca_preprocessing\n","from test_pre_processing import test_data_preparation, test_hog_pca_preprocessing\n","from a1 import A1\n","from a2 import A2\n","from b1 import B1\n","from b2 import B2\n","import tensorflow as tf\n","\n","# set_memory_growth() allocates exclusively the GPU memory needed\n","physical_devices = tf.config.experimental.list_physical_devices('GPU')\n","print(\"Num GPUs Available: \", len(physical_devices))\n","if len(physical_devices) is not 0:\n","    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n","\n","# A1 ===================================================================================================================\n","# Extract smiles for A2 task before dividing all the images in 'celeba' in training and test images.\n","data_directory, faces_not_detected = smiles_extraction(dataset_name='celeba')\n","test_directory, faces_not_detected1 = smiles_extraction(dataset_name='celeba_test')\n","\n","training_batches, valid_batches, test_batches = data_preprocessing(data_directory='celeba', img_size=(96, 96),\n","                                                                   filename_column='img_name', target_column='gender',\n","                                                                   training_percentage_size=0.85, batches_size=16,\n","                                                                   validation_split=0.15)\n","input_shape = training_batches.image_shape\n","# Build model object.\n","model_A1 = A1(input_shape)\n","# Train model based on the training set\n","acc_A1_train, acc_A1_valid = model_A1.train(training_batches, valid_batches, epochs=25, verbose=2, plot=True)\n","# Test model based on the test set.\n","acc_A1_test = model_A1.test(test_batches, verbose=1, confusion_mesh=True)\n","# Test the model on the second larger test dataset provided\n","test_batches = test_data_preparation('celeba_test', filename_column='img_name', target_column='gender',\n","                                     batches_size=16, img_size=(96, 96))\n","acc_A1_test2 = model_A1.test(test_batches, verbose=1, confusion_mesh=False)\n","# Clean up memory\n","del model_A1, physical_devices, faces_not_detected, faces_not_detected1\n","\n","# A2 SVM ===============================================================================================================\n","x_test, x_train, x_valid, y_test, y_train, y_valid, pca, sc = hog_pca_preprocessing(dataset_name=data_directory,\n","                                                                                    img_size=(96, 48),\n","                                                                                    validation_split=0.15,\n","                                                                                    variance=0.90, training_size=0.85,\n","                                                                                    target_column='smiling')\n","# Build model object.\n","# (gamma,c) and tol values are found through grid_search.py and training_A2_plot.py (in the _Additional_code folder).\n","model_A2 = A2(kernel='rbf', gamma=0.001, c=0.5, tol=0.1, verbose=False)\n","# Train model based on the training set\n","acc_A2_train, acc_A2_valid = model_A2.train(x_train, x_valid, y_train, y_valid)\n","# Test model based on the test set.\n","acc_A2_test = model_A2.test(x_test, y_test, confusion_mesh=True)\n","# Test the model on the second larger test dataset provided\n","x_test, y_test = test_hog_pca_preprocessing(test_directory, pca, sc, img_size=(96, 48), target_column='smiling')\n","acc_A2_test2 = model_A2.test(x_test, y_test, confusion_mesh=False)\n","# Clean up memory\n","del x_test, x_train, x_valid, y_test, y_train, y_valid, data_directory, model_A2, pca, sc\n","\n","# B1 ===================================================================================================================\n","training_batches, valid_batches, test_batches = data_preprocessing(data_directory='cartoon_set',\n","                                                                   filename_column='file_name',\n","                                                                   target_column='face_shape', img_size=(224, 224),\n","                                                                   training_percentage_size=0.8, horizontal_flip=False,\n","                                                                   batches_size=16, validation_split=0.2)\n","input_shape = training_batches.image_shape\n","# Build model object.\n","model_B1 = B1(input_shape)\n","# Train model based on the training set\n","acc_B1_train, acc_B1_valid = model_B1.train(training_batches, valid_batches, epochs=10, verbose=2, plot=True)\n","# Test model based on the test set.\n","acc_B1_test = model_B1.test(test_batches, verbose=1, confusion_mesh=True)\n","# Test the model on the second larger test dataset provided\n","test_batches = test_data_preparation('cartoon_set_test', filename_column='file_name', target_column='face_shape',\n","                                     batches_size=16, img_size=(224, 224))\n","acc_B1_test2 = model_B1.test(test_batches, verbose=1, confusion_mesh=False)\n","\n","# B2 ===================================================================================================================\n","# Execute after the B1 Task\n","delete_glasses(dataset_name='cartoon_set', img_size=(224, 224))\n","training_batches, valid_batches, test_batches = data_preprocessing(data_directory='cartoon_set',\n","                                                                   filename_column='file_name',\n","                                                                   target_column='eye_color', img_size=(224, 224),\n","                                                                   training_percentage_size=0.8, batches_size=16,\n","                                                                   horizontal_flip=False, validation_split=0.2)\n","input_shape = training_batches.image_shape\n","# Build model object.\n","model_B2 = B2(input_shape)\n","# Train model based on the training set\n","acc_B2_train, acc_B2_valid = model_B2.train(training_batches, valid_batches, epochs=10, verbose=2, plot=True)\n","# Test model based on the test set.\n","acc_B2_test = model_B2.test(test_batches, verbose=1, confusion_mesh=True)\n","# Test the model on the second larger test dataset provided\n","test_batches = test_data_preparation('cartoon_set_test', filename_column='file_name', target_column='eye_color',\n","                                     batches_size=16, img_size=(224, 224))\n","acc_B2_test2 = model_B2.test(test_batches, verbose=1, confusion_mesh=False)\n","\n","# RESULTS ==============================================================================================================\n","# Print out your results with following format:\n","print('Task  {:\u003c12} {:\u003c12} {:\u003c12} {:\u003c12}\\n'.format('Train Acc', 'Valid Acc', 'Test Acc', 'Test 2 Acc'),\n","      'A1:  {:\u003c12.4f} {:\u003c12.4f} {:\u003c12.4f} {:\u003c12.4f}\\n'.format(acc_A1_train, acc_A1_valid, acc_A1_test, acc_A1_test2),\n","      'A2:  {:\u003c12.4f} {:\u003c12.4f} {:\u003c12.4f} {:\u003c12.4f}\\n'.format(acc_A2_train, acc_A2_valid, acc_A2_test, acc_A2_test2),\n","      'B1:  {:\u003c12.4f} {:\u003c12.4f} {:\u003c12.4f} {:\u003c12.4f}\\n'.format(acc_B1_train, acc_B1_valid, acc_B1_test, acc_B1_test2),\n","      'B2:  {:\u003c12.4f} {:\u003c12.4f} {:\u003c12.4f} {:\u003c12.4f}\\n'.format(acc_B2_train, acc_B2_valid, acc_B2_test, acc_B2_test2))\n"]},{"cell_type":"markdown","metadata":{"id":"bhR3_OY0Aykv"},"source":["# New Section"]},{"cell_type":"markdown","metadata":{"id":"F-vLPGOLAz1B"},"source":["# New Section"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zcMPjSza_2gs"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}